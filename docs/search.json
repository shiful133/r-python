[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are passionate about leveraging the power of geospatial data to gain valuable insights, make informed decisions, and drive innovation. Our mission is to harness the capabilities of both R and Python for geospatial analysis unlocking the potential hidden within location-based information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Geospatial analysis is a powerful field that enables us to study, interpret, and visualize data that is related to geographic locations. It involves the exploration and manipulation of data with spatial attributes, such as latitude, longitude, and elevation. Geospatial analysis finds applications in diverse domains, including environmental monitoring, urban planning, agriculture, epidemiology, and more.\nR and Python are two popular programming languages that offer extensive support for geospatial analysis. Each language has a rich ecosystem of libraries and tools specifically designed to handle spatial data, making them the top choices for geospatial enthusiasts, researchers, and data analysts.\nR, a language initially developed for statistical computing and graphics, has grown to include a plethora of geospatial packages, such as sf, sp, and raster, which allow users to handle, visualize, and analyze spatial data seamlessly. With its straightforward syntax and strong statistical capabilities, R is an excellent option for those looking to combine advanced analytical techniques with spatial data.\nOn the other hand, Python, known for its versatility and ease of use, has become a prominent language for geospatial analysis due to libraries like Geopandas, Folium, Rasterio, and PySAL. Python’s integration with other scientific libraries, such as NumPy, Pandas, and Matplotlib, further enhances its capability to handle and analyze spatial data.\nIn this exploration of geospatial analysis in R and Python, we will delve into the fundamental concepts of working with spatial data, explore different data formats, learn how to manipulate and process geospatial datasets, perform spatial operations, and create visually appealing maps and plots. Additionally, we will showcase some real-world use cases to highlight the practical applications of geospatial analysis in various domains.\nWhether you are an environmental scientist aiming to analyze climate patterns, an urban planner seeking insights for better city development, or simply curious about the fascinating world of geospatial data, this journey through geospatial analysis in R and Python will equip you with the skills and knowledge to unlock valuable insights from spatial data and make informed decisions based on geographic context. Let’s embark on this exciting adventure together and harness the power of geospatial analysis with R and Python!"
  },
  {
    "objectID": "data-export-import.html",
    "href": "data-export-import.html",
    "title": "Data Import Export",
    "section": "",
    "text": "Reading data for analysis and exporting the results to another system for report writing can be done efficiently with R. There are multiple ways to import and export data to/from R. In this tutorial, you will learn some most common ways to read and write data with R."
  },
  {
    "objectID": "data-import-export.html",
    "href": "data-import-export.html",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "Importing and exporting data in Google Colab, which is a popular platform for working with Python notebooks, involves using various libraries and methods to handle different file formats. Here’s a brief guide on how to perform data import and export in Python Colab:\n\n\nIn Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\n\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\n\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\n\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\n\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']\n\n\n\n\n\n\n\n\nYou can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\n\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\n\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\n\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "data-import-export.html#working-directory-1",
    "href": "data-import-export.html#working-directory-1",
    "title": "Data Import-Export in Python",
    "section": "Working directory",
    "text": "Working directory\nIt would be best if you created a working directory in R to read and write files locally.\nBefore creating a working directory, you may check the directory of your current R session; the function getwd() will print the current working directory path as a string.\n```{r}\ngetwd()\n```\nThe following example shows how to create the working directory in R."
  },
  {
    "objectID": "docs/Titanic_Data_ggplot2.html",
    "href": "docs/Titanic_Data_ggplot2.html",
    "title": "Geospatial Analysis in R and Python",
    "section": "",
    "text": "!pip uninstall rpy2 -y\n!pip install rpy2==3.5.1\n%load_ext rpy2.ipython\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n%%R\n#assign(\".lib.loc\", \"drive/MyDrive/R/R_Packages/\", envir = environment(.libPaths))\nassign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n#assign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/local/lib/R/site-library\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n\n.libPaths()\n\n[1] \"drive/MyDrive/R/R_Packages/\" \"/usr/lib/R/site-library\"    \n[3] \"/usr/lib/R/library\"         \n\n\n\n%%R\nlibrary(ggplot2)\nlibrary(tidyverse)\ntitanic <- read.csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\nhead(titanic)\n\n  PassengerId Survived Pclass\n1           1        0      3\n2           2        1      1\n3           3        1      3\n4           4        1      1\n5           5        0      3\n6           6        0      3\n                                                 Name    Sex Age SibSp Parch\n1                             Braund, Mr. Owen Harris   male  22     1     0\n2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                              Heikkinen, Miss. Laina female  26     0     0\n4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                            Allen, Mr. William Henry   male  35     0     0\n6                                    Moran, Mr. James   male  NA     0     0\n            Ticket    Fare Cabin Embarked\n1        A/5 21171  7.2500              S\n2         PC 17599 71.2833   C85        C\n3 STON/O2. 3101282  7.9250              S\n4           113803 53.1000  C123        S\n5           373450  8.0500              S\n6           330877  8.4583              Q\n\n\n\n%%R\n#setup factors\ntitanic$Pclass <- as.factor(titanic$Pclass)\ntitanic$Survived <- as.factor(titanic$Survived)\ntitanic$Sex <- as.factor(titanic$Sex)\ntitanic$Embarked <- as.factor(titanic$Embarked)\n\n\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\nprop.table(table(titanic$Survived))\n\n\n        0         1 \n0.6161616 0.3838384 \n\n\n\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Pclass, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  facet_wrap(~ Pclass)+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Survived, y=Age))+\n  geom_boxplot()+\n  theme_bw()+\n  labs(y=\"Age\",\n       x = \"Survived\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_density(alpha=.5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(alpha=.5,binwidth=5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n%%R\n#install.packages(\"rgee\", lib='drive/MyDrive/R/R_Packages/')\nlibrary(rgee)\nlibrary(sf)\nlibrary(raster)"
  },
  {
    "objectID": "Titanic_Data_ggplot2.html",
    "href": "Titanic_Data_ggplot2.html",
    "title": "Geospatial Analysis in R and Python",
    "section": "",
    "text": "print('Hello World!')\n\nHello World!\n\n\n\n!pip uninstall rpy2 -y\n!pip install rpy2==3.5.1\n%load_ext rpy2.ipython\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n%%R\n#assign(\".lib.loc\", \"drive/MyDrive/R/R_Packages/\", envir = environment(.libPaths))\nassign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n#assign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/local/lib/R/site-library\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n\n.libPaths()\n\n[1] \"drive/MyDrive/R/R_Packages/\" \"/usr/lib/R/site-library\"    \n[3] \"/usr/lib/R/library\"         \n\n\n\n%%R\nlibrary(ggplot2)\nlibrary(tidyverse)\ntitanic <- read.csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\nhead(titanic)\n\n  PassengerId Survived Pclass\n1           1        0      3\n2           2        1      1\n3           3        1      3\n4           4        1      1\n5           5        0      3\n6           6        0      3\n                                                 Name    Sex Age SibSp Parch\n1                             Braund, Mr. Owen Harris   male  22     1     0\n2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                              Heikkinen, Miss. Laina female  26     0     0\n4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                            Allen, Mr. William Henry   male  35     0     0\n6                                    Moran, Mr. James   male  NA     0     0\n            Ticket    Fare Cabin Embarked\n1        A/5 21171  7.2500              S\n2         PC 17599 71.2833   C85        C\n3 STON/O2. 3101282  7.9250              S\n4           113803 53.1000  C123        S\n5           373450  8.0500              S\n6           330877  8.4583              Q\n\n\n\n%%R\n#setup factors\ntitanic$Pclass <- as.factor(titanic$Pclass)\ntitanic$Survived <- as.factor(titanic$Survived)\ntitanic$Sex <- as.factor(titanic$Sex)\ntitanic$Embarked <- as.factor(titanic$Embarked)\n\n\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\nprop.table(table(titanic$Survived))\n\n\n        0         1 \n0.6161616 0.3838384 \n\n\n\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Pclass, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  facet_wrap(~ Pclass)+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Survived, y=Age))+\n  geom_boxplot()+\n  theme_bw()+\n  labs(y=\"Age\",\n       x = \"Survived\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_density(alpha=.5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(alpha=.5,binwidth=5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n%%R\n#install.packages(\"rgee\", lib='drive/MyDrive/R/R_Packages/')\nlibrary(rgee)\nlibrary(sf)\nlibrary(raster)"
  },
  {
    "objectID": "R_Titanic_Data_ggplot2.html",
    "href": "R_Titanic_Data_ggplot2.html",
    "title": "Geospatial Analysis in R and Python",
    "section": "",
    "text": "Code\n!pip uninstall rpy2 -y\n!pip install rpy2==3.5.1\n%load_ext rpy2.ipython\n\n\n\n\nCode\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nMounted at /content/drive\n\n\n\n\nCode\n%%R\n#assign(\".lib.loc\", \"drive/MyDrive/R/R_Packages/\", envir = environment(.libPaths))\nassign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n#assign(\".lib.loc\", c(\"drive/MyDrive/R/R_Packages/\",\"/usr/local/lib/R/site-library\",\"/usr/lib/R/site-library\",\"/usr/lib/R/library\"), envir = environment(.libPaths))\n\n.libPaths()\n\n\n[1] \"drive/MyDrive/R/R_Packages/\" \"/usr/lib/R/site-library\"    \n[3] \"/usr/lib/R/library\"         \n\n\n\n\nCode\n%%R\nlibrary(ggplot2)\nlibrary(tidyverse)\ntitanic &lt;- read.csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\nhead(titanic)\n\n\n  PassengerId Survived Pclass\n1           1        0      3\n2           2        1      1\n3           3        1      3\n4           4        1      1\n5           5        0      3\n6           6        0      3\n                                                 Name    Sex Age SibSp Parch\n1                             Braund, Mr. Owen Harris   male  22     1     0\n2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                              Heikkinen, Miss. Laina female  26     0     0\n4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                            Allen, Mr. William Henry   male  35     0     0\n6                                    Moran, Mr. James   male  NA     0     0\n            Ticket    Fare Cabin Embarked\n1        A/5 21171  7.2500              S\n2         PC 17599 71.2833   C85        C\n3 STON/O2. 3101282  7.9250              S\n4           113803 53.1000  C123        S\n5           373450  8.0500              S\n6           330877  8.4583              Q\n\n\n\n\nCode\n%%R\n#setup factors\ntitanic$Pclass &lt;- as.factor(titanic$Pclass)\ntitanic$Survived &lt;- as.factor(titanic$Survived)\ntitanic$Sex &lt;- as.factor(titanic$Sex)\ntitanic$Embarked &lt;- as.factor(titanic$Embarked)\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Survived))+\ngeom_bar()\n\nprop.table(table(titanic$Survived))\n\n\n\n        0         1 \n0.6161616 0.3838384 \n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Pclass, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Sex, fill=Survived))+\n  geom_bar()+\n  theme_bw()+\n  facet_wrap(~ Pclass)+\n  labs(y=\"Number\",\n       title=\"Titanic Survival Rate\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Age))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(binwidth=5)+\n  theme_bw()+\n  labs(y=\"Number\",\n       x = \"Age (binwidth = 5)\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Survived, y=Age))+\n  geom_boxplot()+\n  theme_bw()+\n  labs(y=\"Age\",\n       x = \"Survived\",\n       title=\"Titanic Age Distribution\",\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_density(alpha=.5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n\n\nCode\n%%R\nggplot(titanic, aes(x = Age, fill=Survived))+\n  geom_histogram(alpha=.5,binwidth=5)+\n  theme_bw()+\n  facet_wrap(Sex~Pclass)+\n  labs(y=\"Number\",\n       x = \"Age\",\n       title=\"Titanic Survival Rate by Age, Pclass, Sex\"\n       )\n\n\n\n\n\n\n\nCode\n%%R\n#install.packages(\"rgee\", lib='drive/MyDrive/R/R_Packages/')\nlibrary(rgee)\nlibrary(sf)\nlibrary(raster)"
  },
  {
    "objectID": "data-import-export.html#working-directory-1-1",
    "href": "data-import-export.html#working-directory-1-1",
    "title": "Data Import-Export in Python",
    "section": "Working directory 1",
    "text": "Working directory 1\nIt would be best if you created a working directory in R to read and write files locally.\nBefore creating a working directory, you may check the directory of your current R session; the function getwd() will print the current working directory path as a string."
  },
  {
    "objectID": "data-import-export.html#working-directory",
    "href": "data-import-export.html#working-directory",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "In Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\n\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\n\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\n\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\n\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']"
  },
  {
    "objectID": "Python_Data_Import_Export.html",
    "href": "Python_Data_Import_Export.html",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "Importing and exporting data in Google Colab, which is a popular platform for working with Python notebooks, involves using various libraries and methods to handle different file formats. Here’s a brief guide on how to perform data import and export in Python Colab:"
  },
  {
    "objectID": "docs/Python_Data_Import_Export.html",
    "href": "docs/Python_Data_Import_Export.html",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "Importing and exporting data in Google Colab, which is a popular platform for working with Python notebooks, involves using various libraries and methods to handle different file formats. Here’s a brief guide on how to perform data import and export in Python Colab:"
  },
  {
    "objectID": "data-import-export.html#working-directory-2",
    "href": "data-import-export.html#working-directory-2",
    "title": "Data Import-Export in Python",
    "section": "Working Directory",
    "text": "Working Directory\nIn Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\nGet Current Working Directory:\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\nMounting Google Drive:\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nMounted at /content/drive\n\n\n\n\nChange Working Directory:\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\nCheck Files in the Directory:\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory"
  },
  {
    "objectID": "Python_Data_Import_Export.html#working-directory",
    "href": "Python_Data_Import_Export.html#working-directory",
    "title": "Data Import-Export in Python",
    "section": "Working Directory",
    "text": "Working Directory\nIn Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\nGet Current Working Directory:\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\nMounting Google Drive:\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\nChange Working Directory:\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\nCheck Files in the Directory:\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']"
  },
  {
    "objectID": "data-import-export.html#working-directory-1-2",
    "href": "data-import-export.html#working-directory-1-2",
    "title": "Data Import-Export in Python",
    "section": "Working directory 1",
    "text": "Working directory 1\nIt would be best if you created a working directory in R to read and write files locally.\nBefore creating a working directory, you may check the directory of your current R session; the function getwd() will print the current working directory path as a string."
  },
  {
    "objectID": "data-import-export.html#reading-csv-files",
    "href": "data-import-export.html#reading-csv-files",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "You can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\n\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n###Reading files by uploading:\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\n\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\n\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "data-import-export.html#readimport-data-into-python",
    "href": "data-import-export.html#readimport-data-into-python",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "You can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\n\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\n\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\n\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "data-import-export.html#plot",
    "href": "data-import-export.html#plot",
    "title": "Data Import-Export in Python",
    "section": "Plot",
    "text": "Plot\nReading data for analysis and exporting the results to another system for report writing can be done efficiently with R. There are multiple ways to import and export data to/from R. In this tutorial, you will learn some most common ways to read and write data with R."
  },
  {
    "objectID": "python-data-import-export.html",
    "href": "python-data-import-export.html",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "Importing and exporting data in Google Colab, which is a popular platform for working with Python notebooks, involves using various libraries and methods to handle different file formats. Here’s a brief guide on how to perform data import and export in Python Colab:\n\n\nIn Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\n\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\n\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\n\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\n\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']\n\n\n\n\n\n\n\n\nYou can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\n\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\n\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\n\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "python-data-import-export.html#working-directory",
    "href": "python-data-import-export.html#working-directory",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "In Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\n\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\n\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\n\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\n\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']"
  },
  {
    "objectID": "python-data-import-export.html#readimport-data-into-python",
    "href": "python-data-import-export.html#readimport-data-into-python",
    "title": "Data Import-Export in Python",
    "section": "",
    "text": "You can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\n\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\n\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\n\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "Python_Data_Import_Export.html#readimport-data-into-python",
    "href": "Python_Data_Import_Export.html#readimport-data-into-python",
    "title": "Data Import-Export in Python",
    "section": "Read/Import Data into Python:",
    "text": "Read/Import Data into Python:\n\nReading CSV Files:\nYou can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\nRead from Google Drive:\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nRead from URL:\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nReading XLSX Files:\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nReading files by uploading:\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nShow column names\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\nReading JSON Files:\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\nRead Stata Data Files (.dta):\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "python-install-packages-google-drive.html",
    "href": "python-install-packages-google-drive.html",
    "title": "Install Packages in Google Drive",
    "section": "",
    "text": "In Google Colab, the environment resets after each session, which means that any custom installed packages will not persist between sessions.\nIn each new session, you will need to install your custom packages again. You can do this by running the following command at the beginning of your Colab notebook:\n!pip install package_name #Replace package_name with the name of the package you want to install.\n\n\nMounting Google Drive:\nIf you want to persist data, files, or installed packages across different Colab sessions, you can mount your Google Drive. This way, you can save files and packages on your Google Drive, which will be accessible in subsequent sessions.\n\n\nCode\n# Mounting Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nMounted at /content/drive\n\n\n\nWe can see the filelist in any directory using os.listdir() command.\n\n\nCode\nimport os\nos.listdir('/content/drive/MyDrive/Python/')\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv',\n 'packages',\n '.ipynb_checkpoints']\n\n\n\nIn Python, sys.path is a list that represents the search path for modules. When you import a module in Python, the interpreter searches for that module in the directories listed in sys.path. It’s essentially a list of directory paths where Python looks for modules when you use the import statement.\n\n\nCode\nimport sys\n# Show the list of existing paths\nprint(sys.path)\n\n\n['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n\n\n\nNow we will add our custom paths to the sys.path using sys.path.append command.\n\n\nCode\nimport sys\n\nsys.path.append('/content/drive/MyDrive/Python/packages/')\n# Custom path is added to the list\nprint(sys.path)\n\n\n\n['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Python/packages/']\n\n\n\nNow we can install packages to our custom location. Here we will install pyreadstat.\n\n\nCode\n# Uncomment the following code to install now\n#!pip install pyreadstat --target=/content/drive/MyDrive/Python/packages\n\n\n\n\nCode\nimport pyreadstat\n\n\nN.B. You need to mount Google Drive and add the custom location in sys.path in every colab notebook using following command.\n#Mounting Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport sys\nsys.path.append('/content/drive/MyDrive/Python/packages/') #Replace with your Google Drive path\nprint(sys.path)"
  },
  {
    "objectID": "python_read_write_spatial_data.html",
    "href": "python_read_write_spatial_data.html",
    "title": "Read/Write Spatial Data",
    "section": "",
    "text": "Zia Ahmed, University at Buffalo"
  },
  {
    "objectID": "python_read_write_spatial_data.html#vector-data",
    "href": "python_read_write_spatial_data.html#vector-data",
    "title": "Read/Write Spatial Data",
    "section": "Vector Data",
    "text": "Vector Data\nVector data is a type of geospatial data representation used in Geographic Information Systems (GIS) and computer graphics. It represents spatial information using points, lines, and polygons, which are defined by their geometric coordinates. These coordinates consist of x and y (and sometimes z) values that describe the location of each point, the vertices of lines, or the corners of polygons.\nHere’s a breakdown of the main elements of vector data:\n\nPoints: A single data point that represents a specific location in space, defined by its x and y coordinates (and potentially z for 3D data). Points are often used to represent features like cities, landmarks, or sampling locations.\nLines (Polylines): A series of connected points, which can be straight or curved, used to represent linear features such as roads, rivers, or boundaries. Lines are composed of multiple vertices that define their shape.\nPolygons: Closed shapes formed by connecting a series of lines to enclose an area. Polygons are used to represent features like countries, lakes, forests, and other spatially bounded regions.\n\nVector data provides a more detailed and accurate representation of geographic features compared to raster data, which is another common data format used in GIS. One of the significant advantages of vector data is that it allows for precise measurements, editing, and analysis of geographic features. It is also more scalable and efficient for storing data with well-defined boundaries and discrete features.\nSome common vector data formats include Shapefile (.shp), GeoJSON (.geojson), and Keyhole Markup Language (.kml). These formats can store various attributes along with the spatial coordinates, making it possible to attach additional information (e.g., population data for cities or elevation data for terrain) to each feature represented in the vector dataset.\nThe shapefile, a interchangeably data format which are regulated by ESRI. It is one of the most common form of geospatial vector data used in GIS software and analyses. Three unique files are required for a shapefile, including: .shx (shape index format; this tags the shapefile with a position, so users can move it forward and backward among layers, a .shp (shape format, which stores geometric data, and a .dbf (attribute format file; which holds attributes (information) for the shapes in the file)."
  },
  {
    "objectID": "python_read_write_spatial_data.html#raster-data",
    "href": "python_read_write_spatial_data.html#raster-data",
    "title": "Read/Write Spatial Data",
    "section": "Raster data",
    "text": "Raster data\nRaster data is another type of geospatial data representation used in Geographic Information Systems (GIS) and various other applications. Unlike vector data, which uses points, lines, and polygons to represent spatial information, raster data organizes information in a grid-like structure, with each cell or pixel containing a value that represents a specific attribute or measurement at a particular location.\nKey characteristics of raster data include:\n\nPixel-based representation: Raster data divides the geographic area into a regular grid of pixels, where each pixel corresponds to a specific location on the Earth’s surface. Each pixel stores a value representing a particular attribute, such as elevation, temperature, land use, satellite imagery, or any other continuous data.\nResolution: The resolution of raster data refers to the size of each pixel, which determines the level of detail in the data. Higher resolution means smaller pixel sizes and more detail, while lower resolution means larger pixel sizes and less detail.\nCell values: The cell values in raster data can represent a wide range of information, such as elevation above sea level, temperature readings, land cover categories, or satellite sensor measurements.\nCoverage and extent: Raster datasets have a defined coverage area and extent, meaning they represent a specific geographic region with boundaries.\n\nCommon examples of raster data include digital elevation models (DEMs) used for terrain representation, satellite imagery, aerial photographs, climate data, and remotely sensed data from various sensors.\nRaster data has several advantages, including its ability to efficiently store and process large continuous datasets and perform spatial analysis like interpolation, proximity analysis, and slope calculations. However, it may not be as accurate or suitable for representing discrete features with well-defined boundaries compared to vector data.\nRaster data formats often include GeoTIFF (.tif), JPEG (.jpg), PNG (.png), and many other specialized formats depending on the type of data being represented and the intended use.\nUnlike vector data, a raster data consists of cells or pixels organized into rows and columns as a matrix where each cell contains a value representing geographical feature on the earth. The size of the area on the surface that each pixel covers is known as the spatial resolution of the image. For instance, an image that has a 1 m spatial resolution means that each pixel in the image represents a 1 m x 1 m area. There are two types of raster data: continuous and discrete. An example of discrete raster data is Land use raster. Data types are flexible, including discrete and categorical data, such as soil or land-use maps, or continuous data as in digital elevation models, precipitation gradient maps, or pollutant concentration maps, etc.\n\nGenerally, two types of raster use in GIS and remote sensing application: a single band, or layer measure of a single characteristic of spatial area and multiple bands raster contains multiple spatially corresponding matrices of cell values representing the same spatial area. An example of a single-band raster data set is a digital elevation model (DEM). Each cell in a DEM contains only one value representing elevation of earth surface. Most satellite imagery has multiple bands, typically containing values within band of the electromagnetic spectrum."
  },
  {
    "objectID": "python_read_write_spatial_data.html#r-packages-for-spatial-read-and-write",
    "href": "python_read_write_spatial_data.html#r-packages-for-spatial-read-and-write",
    "title": "Read/Write Spatial Data",
    "section": "R Packages for Spatial Read and Write",
    "text": "R Packages for Spatial Read and Write\n\nsf\nThe “sf” package provides a consistent and efficient set of tools for handling spatial data, including reading, writing, manipulating, and visualizing spatial data. It also supports various spatial operations and is compatible with many other R packages that deal with spatial data.\nSome of the key functionalities of the “sf” package include:\n\nReading and Writing: Loading and saving spatial data in various formats, such as shapefiles, GeoJSON, and spatial databases.\nData Manipulation: Working with spatial data alongside traditional data frames, allowing for easy filtering, subsetting, and merging of data.\nSpatial Operations: Performing geometric operations like intersections, unions, and buffering on spatial data.\nVisualization: Creating maps and visualizing spatial data using functions that work well with ggplot2.\nCoordinate Transformation: Converting spatial data between different coordinate reference systems (CRS) and projecting data onto different map projections.\n\n\ninstall.packages(‘sf’)\n\n\n\nrgdal\nIn the context of R, “rgdal” is a package that provides bindings to the Geospatial Data Abstraction Library (GDAL) for reading, writing, and manipulating geospatial raster and vector data formats. GDAL is a powerful open-source library that supports a wide range of geospatial data formats commonly used in the GIS (Geographic Information Systems) and remote sensing domains.\nThe “rgdal” package allows R users to work with spatial data in various formats, including but not limited to shapefiles, GeoTIFFs, NetCDF, and more. It enables data import, export, and transformation between different coordinate reference systems (CRS). The package also facilitates the extraction of geospatial information, such as spatial extents and coordinate information.\nImportant functions and features provided by “rgdal” include:\n\nData Import and Export: Reading and writing geospatial data to and from various formats using functions like readOGR, writeOGR, readGDAL, writeGDAL, etc.\nCoordinate Transformation: Converting data between different CRS using spTransform and project.\nSpatial Subsetting: Subsetting and clipping spatial data based on bounding boxes or other spatial objects.\nData Information: Extracting metadata and other geospatial information from datasets.\n\nTo use the “rgdal” package in R, you need to install it along with its dependencies. However, it’s important to note that installing the “rgdal” package can be tricky due to the system-level dependencies required by GDAL. Depending on your operating system (Windows, macOS, or Linux), you might need to install additional libraries or packages before successfully installing “rgdal.”\n\ninstall.packages(‘rgdal’)\n\n\n\nraster\nthe “raster” package is an important R package used for working with raster data, which represents data in grid format (e.g., satellite images, digital elevation models, climate data) rather than vector format (e.g., points, lines, polygons). The package provides a wide range of functionalities for reading, writing, manipulating, and analyzing raster datasets.\nThe “raster” package is built on top of the “sp” and “rgdal” packages, making it easy to integrate raster data with spatial data. It offers efficient and memory-friendly methods for handling large raster datasets and supports various raster file formats, including GeoTIFF, NetCDF, and more.\nSome of the key functionalists of the “raster” package include:\n\nData Import and Export: Reading and writing raster data from and to various file formats using functions like raster(), brick(), writeRaster(), and more.\nData Manipulation: Resampling, cropping, masking, and reprojecting raster data.\nMathematical Operations: Performing mathematical operations on raster data, such as map algebra, cell statistics, and overlay operations.\nZonal Statistics: Calculating statistics (e.g., mean, sum, max) within defined zones or polygons.\nVisualization: Creating various types of plots and visualizations for raster data.\nTime Series Analysis: Handling and analyzing raster time series data.\n\n\ninstall.packages(‘raster’)\n\nIn this exercise, you will learn how to read and write the following types of spatial data with R.\n\nVector data\n\nPolygon data\nPoints Vector\nLines or Polylines\n\nRaster data\n\nSingle band raster\nMulti-bands raster\n\nGeoJson format\nKML format\n\n\n\nInstall rpy2\n\n\nCode\n!pip uninstall rpy2 -y\n!pip install rpy2==3.5.1\n%load_ext rpy2.ipython\n\n\nFound existing installation: rpy2 3.4.2\nUninstalling rpy2-3.4.2:\n  Successfully uninstalled rpy2-3.4.2\nCollecting rpy2==3.5.1\n  Downloading rpy2-3.5.1.tar.gz (201 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201.7/201.7 kB 3.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: cffi&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (1.15.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (3.1.2)\nRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (2022.7.1)\nRequirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.1) (5.0.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.10.0-&gt;rpy2==3.5.1) (2.21)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;rpy2==3.5.1) (2.1.3)\nBuilding wheels for collected packages: rpy2\n  Building wheel for rpy2 (setup.py) ... done\n  Created wheel for rpy2: filename=rpy2-3.5.1-cp310-cp310-linux_x86_64.whl size=314932 sha256=ad23cb4a2e5d5ec96981258322d07768cde91462933e4324cc44d7bcbd580206\n  Stored in directory: /root/.cache/pip/wheels/73/a6/ff/4e75dd1ce1cfa2b9a670cbccf6a1e41c553199e9b25f05d953\nSuccessfully built rpy2\nInstalling collected packages: rpy2\nSuccessfully installed rpy2-3.5.1\n\n\n\n\nMount Google Drive\n\n\nCode\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\nInstall Packages\n\n\nCode\n%%R\ninstall.packages(c(\"sf\", 'raster', 'rgdal'), lib='drive/My Drive/R/')\n\n\n\n\nCode\n%%R\ninstall.packages('devtools', lib='drive/My Drive/R/')\n\n\n\n\nCode\n%%R\nlibrary(devtools)\ninstall_github(\"bleutner/RStoolbox\", lib='drive/My Drive/R/')\n\n\n\n\nCode\n%%R\ninstall.packages(\"geojsonsf\", lib='drive/My Drive/R/')\n\n\n\n\nCode\n%%R\ninstall.packages(c(\"mapview\", 'geojson'), lib='drive/My Drive/R/')\n\n\n\n\nLoad library\n\n\nCode\n%%R\n.libPaths('drive/My Drive/R')\nlibrary(sf)       # spatial vector data\nlibrary (raster)    # raster and vector data read & write\nlibrary (rgdal)     # raster and vector data read & write\n\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Loading required package: sp\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Please note that rgdal will be retired during October 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-7, (SVN revision 1203)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.4.3, released 2022/04/22\nPath to GDAL shared files: /usr/share/gdal\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821]\nPath to PROJ shared files: /root/.local/share/proj:/usr/share/proj\nPROJ CDN enabled: TRUE\nLinking to sp version:2.0-0\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\n\n\n\n\nData\nIn this exercise we use following data set:\n\nSpatial polygon of the district of Bangladesh (bd_district.shp)\nSpatial point data of soil sampling location under Rajshahi Division of Bangladesh (raj_soil_data_GCS.shp)\nSpatial line of road net work of Rajshai division (raj_road_GCS.shp)\nSRTM DEM raster of Rajshahi division (raj_DEM_GCS.tif)\nOne tile of Landsat 9 for Rajshahi Division\n\nAll data could be found here for download.\nBefore reading the data from a local drive, you need to define a working directory from where you want to read or to write data. You may use setwd() function to create a working directory. Or we can define a path for data outside of our working directory from where we can read files. In this case, we will use paste0(data path, “file name”). I will load data from my Google drive or directly from my github repository."
  },
  {
    "objectID": "python_read_write_spatial_data.html#read-and-write-vector-data",
    "href": "python_read_write_spatial_data.html#read-and-write-vector-data",
    "title": "Read/Write Spatial Data",
    "section": "Read and Write Vector Data",
    "text": "Read and Write Vector Data\n\nPoylgons\nFor reading ESRI shape file, we can use either readOGR() of rgdal or shapefile of raster packages to read shape file from your local drive.\n\n\nCode\n%%R\n# reading shape file with raster or rgdal package from google drive\ndist&lt;-rgdal::readOGR(\"/content/drive/MyDrive/Data/Bangladesh/Shapefiles/bd_district.shp\")\ndist&lt;-raster::shapefile('/content/drive/MyDrive/Data/Bangladesh/Shapefiles/bd_district.shp')\n\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/content/drive/MyDrive/Data/Bangladesh/Shapefiles/bd_district.shp\", layer: \"bd_district\"\nwith 64 features\nIt has 14 fields\n\n\nYou may also use st_read() from sf package to load shapefile directly from my github using GDAL Virtual File Systems (vsicurl).\n\n\nCode\n%%R\n# Reading shape file  sf package from github\ndist.st = sf::st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/bd_district.shp\")\n\n\nReading layer `bd_district' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/bd_district.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 64 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 20.59061 xmax: 92.68031 ymax: 26.63451\nCRS:           NA\n\n\nWe plot st object using polt() function:\n\n\nCode\n%%R\nplot(dist.st)\n\n\n\n\n\nWe can only plot single layer (geometry)\n\n\nCode\n%%R\nplot(dist.st$geometry, main=\"District of Rajshahi Division\")\n\n\n\n\n\n\nWrite spatial polygons\nWe you can use st_write() to write st objects as shape files\n\n\nCode\n%%R\ndist.sp &lt;- sf:::as_Spatial(dist.st$geom) # This works\nraster::shapefile(dist.sp, \"bd_district.shp\", overwrite=TRUE)\n\n\nWe can also use shapefile function of raster package to write the vector data. but before you have to convert st objects to Spatial Polygon dataframe.\n\n\nCode\n%%R\nsf::st_write(dist.st, \"bd_district.shp\", append=FALSE)\n\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: writing: substituting ENGCRS[\"Undefined Cartesian SRS with unknown unit\"] for missing CRS\n\n\n\nDeleting layer `bd_district' using driver `ESRI Shapefile'\nWriting layer `bd_district' to data source \n  `bd_district.shp' using driver `ESRI Shapefile'\nWriting 64 features with 14 fields and geometry type Multi Polygon.\n\n\n\n\n\nSpatial Point data\nWe will read point shape file containing around 5,000 soil samples points of Rajshahi Division. These samples were collected by the SRDI. We will use shapefile() or st_read() to read this vector data.\n\n\nCode\n%%R\n## Reading point shape file with raster package from google drive\npoint&lt;-raster::shapefile('/content/drive/MyDrive/Data/Bangladesh/Shapefiles/raj_soil_data_GCS.shp')\nprint(proj4string(point.GCS))\n\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n\nCode\n%%R\n# from github\npoint.st = sf::st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_soil_data_GCS.shp\")\n\n\nReading layer `raj_soil_data_GCS' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_soil_data_GCS.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5796 features and 34 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 88.05378 ymin: 23.8279 xmax: 89.81412 ymax: 25.26618\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n%%R\nplot(point.st,\n     main= \"Point Vector map\",\n     pch=20,   # symbol type\n     cex=0.8)  # symbol size\n\n\n\n\n\n\n\nCode\n%%R\nplot(point.st$geometry, main=\"Soil Sample Collecting locations\")\nbox()\n\n\n\n\n\n\n\nLines or Polylines\nOne-dimensional lines, also called polylines, are used to represent geographical features like rivers, roads, railroads, trails, and topographic lines. Note that these features are linear in nature and do not have area like polygons. Hence, they can measure distance.\n\n\nCode\n%%R\n# from google drive\nline&lt;-raster::shapefile('/content/drive/MyDrive/Data/Bangladesh/Shapefiles/raj_road_GCS.shp')      # with raster\n\n\n\n\nCode\n%%R\n# from github\nlines.st = sf::st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_road_GCS.shp\")\n\n\nReading layer `raj_road_GCS' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_road_GCS.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1596 features and 7 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 88.10178 ymin: 23.83185 xmax: 89.80968 ymax: 25.2748\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n%%R\nplot(lines.st)\n\n\n\n\n\n\n\nCode\n%%R\nplot(lines.st$geometry, main=\"Major Road Netork\")\nbox()"
  },
  {
    "objectID": "python_read_write_spatial_data.html#read-and-write-raster-data",
    "href": "python_read_write_spatial_data.html#read-and-write-raster-data",
    "title": "Read/Write Spatial Data",
    "section": "Read and Write Raster Data",
    "text": "Read and Write Raster Data\n\nSingle band raster\nA single band raster, also known as a single-band image or grayscale image, is a type of raster data that contains only one band or layer of information. In a single-band raster, each pixel is represented by a single value, typically indicating a certain property or characteristic at that location. The value of each pixel in the raster corresponds to the magnitude of the property being measured at that specific location. Digital Elevation Models (DEMs) represent the elevation or height values of the Earth’s surface at each pixel location. DEMs are often used in terrain analysis, hydrology, and 3D visualization.\nraster() function of raster package will use to read the single band raster data, such DEM in R\n\n\nCode\n%%R\n# from google drive\ndem = raster::raster(\"/content/drive/MyDrive/Data/Bangladesh/Raster/raj_DEM_GCS.tif\")\nplot(dem)\n\n\n\n\n\n\n\nCode\n%%R\n# from github\ndem= raster::raster(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh/Raster/raj_DEM_GCS.tif\")\ndem\n\n\nclass      : RasterLayer \ndimensions : 625, 768, 480000  (nrow, ncol, ncell)\nresolution : 0.002378588, 0.002382236  (x, y)\nextent     : 87.99895, 89.82571, 23.79179, 25.28068  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : raj_DEM_GCS.tif \nnames      : Band_1 \nvalues     : -10, 54  (min, max)\n\n\n\nWe can write raster as geotiff using writeRaster() function:\n\n\nCode\n%%R\nraster::writeRaster(dem, filename=\"/content/drive/MyDrive/Data/Bangladesh/Raster/raj_DEM_GCS.tif\", formate=\"GTiff\",  overwrite=TRUE)\n\n\n\n\nMulti-bands raster\nA multiband raster, also known as a multi-band image, is a type of raster data that contains multiple bands or layers of information. Each band in the raster represents a different property or characteristic at each pixel location on the Earth’s surface. Multiband rasters are widely used in various fields, including remote sensing, GIS (Geographic Information Systems), and image processing, to capture and analyze complex data that cannot be represented in a single band.\nFor loading multi-bands image in R, we will use a sub-set of Landsat 9 multispectral image covering Onondaga county of New York state. This image was downloaded from USGS Earth Explore.\nYou can read single band image one by one with raster() function.\n\n\nCode\n%%R\n# read band 2 (Blue band)\nb2=raster::raster('/content/drive/MyDrive/Data/Bangladesh/LC09_L2SP_138043_20221216_20221223_02_T1/LC09_L2SP_138043_20221216_20221223_02_T1_SR_B2.TIF')\n# read band 3 (Green band)\nb3=raster::raster('/content/drive/MyDrive/Data/Bangladesh/LC09_L2SP_138043_20221216_20221223_02_T1/LC09_L2SP_138043_20221216_20221223_02_T1_SR_B3.TIF')\n# read band 4 (Red band)\nb4=raster::raster('/content/drive/MyDrive/Data/Bangladesh/LC09_L2SP_138043_20221216_20221223_02_T1/LC09_L2SP_138043_20221216_20221223_02_T1_SR_B4.TIF')\n# read band 5 (NIR band)\nb5=raster::raster('/content/drive/MyDrive/Data/Bangladesh/LC09_L2SP_138043_20221216_20221223_02_T1/LC09_L2SP_138043_20221216_20221223_02_T1_SR_B5.TIF')\n\n\n\nNow, you can use stack() or brick() function to create multi-band raster stack\n\n\nCode\n%%R\nmulti_band&lt;-stack(b2,b3,b4,b5)\nplot(multi_band)\n\n\n\n\n\n\n\nCode\n%%R\n.libPaths('drive/My Drive/R')\nlibrary(RStoolbox)\nlibrary(ggplot2)\n\n\n\n\nCode\n%%R\np1&lt;-ggRGB(multi_band, r=4, g=3, b=2, stretch = \"lin\")+\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())+\n  ggtitle(\"False Color\\n (R= Red, G= Green, B= Blue)\")\np1"
  },
  {
    "objectID": "python_read_write_spatial_data.html#geojson-data",
    "href": "python_read_write_spatial_data.html#geojson-data",
    "title": "Read/Write Spatial Data",
    "section": "GeoJson data",
    "text": "GeoJson data\nGeoJSON is a format for encoding geographic data structures using JavaScript Object Notation (JSON). It is commonly used for representing geographical features and their attributes such as points, lines, polygons, and more. GeoJSON is widely used in web mapping applications and geographic information systems (GIS) due to its simplicity, easy integration with web technologies, and human-readability.\nHere’s an example of a simple GeoJSON representation for a point feature:\n\n\nCode\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [longitude, latitude]\n  },\n  \"properties\": {\n    \"name\": \"Sample Point\",\n    \"description\": \"This is an example of a GeoJSON point feature.\"\n  }\n}\n\n\nHere’s an explanation of the key components:\n\ntype: Specifies the type of the GeoJSON object. It can be “Feature,” “FeatureCollection,” “Point,” “LineString,” “Polygon,” and others.\ngeometry: Contains the geometric data for the feature, such as the coordinates for a point, the array of coordinates for a line or polygon, etc.\ncoordinates: The array of coordinates in the format [longitude, latitude] for points and [ [lon1, lat1], [lon2, lat2], ..., [lonN, latN] ] for lines and polygons.\nproperties: An object that holds additional properties or attributes associated with the feature.\n\nWe use st_read() from sf package read raj_district_GCS.geojson from github using “vsicurl”\n\n\nCode\n%%R\n# from my github\nraj.st = sf::st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_district_GCS.geojson\")\n\n\nReading layer `raj_district_GCS' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_district_GCS.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 8 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 23.80807 xmax: 89.82498 ymax: 25.27759\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n%%R\nplot(raj.st)\n\n\n\n\n\nWe also use geojsonsf to read and write GeoJSON data in R.\n\n\nCode\n%%R\nlibrary(geojsonsf)\nraj.geo = geojson_sf(\"https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_district_GCS.geojson\")\nraj.geo\n\n\nSimple feature collection with 8 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 23.80807 xmax: 89.82498 ymax: 25.27759\nGeodetic CRS:  WGS 84\n  ADM0_PCODE    ADM0_EN ADM2ALT2EN ADM2_REF ADM1_PCODE  ADM1_EN   ADM2_EN\n1         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi     Bogra\n2         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi Joypurhat\n3         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi   Naogaon\n4         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi    Natore\n5         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi Nawabganj\n6         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi     Pabna\n7         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi  Rajshahi\n8         BD Bangladesh       &lt;NA&gt;     &lt;NA&gt;       BD50 Rajshahi Sirajganj\n  ADM2ALT1EN ADM2_PCODE    validOn       date Shape_Area\n1       &lt;NA&gt;     BD5010 2020-11-13 2015-01-01 0.26009774\n2       &lt;NA&gt;     BD5038 2020-11-13 2015-01-01 0.08593419\n3       &lt;NA&gt;     BD5064 2020-11-13 2015-01-01 0.30772759\n4       &lt;NA&gt;     BD5069 2020-11-13 2015-01-01 0.16947264\n5       &lt;NA&gt;     BD5070 2020-11-13 2015-01-01 0.15038568\n6       &lt;NA&gt;     BD5076 2020-11-13 2015-01-01 0.21210274\n7       &lt;NA&gt;     BD5081 2020-11-13 2015-01-01 0.21714336\n8       &lt;NA&gt;     BD5088 2020-11-13 2015-01-01 0.22199195\n                        geometry Shape_Leng ValidTo\n1 MULTIPOLYGON (((89.71534 24...   4.289236    &lt;NA&gt;\n2 MULTIPOLYGON (((89.28009 25...   2.214957    &lt;NA&gt;\n3 MULTIPOLYGON (((88.92302 25...   5.098052    &lt;NA&gt;\n4 MULTIPOLYGON (((89.33903 24...   3.458987    &lt;NA&gt;\n5 MULTIPOLYGON (((88.50557 24...   3.329448    &lt;NA&gt;\n6 MULTIPOLYGON (((89.72158 24...   3.078707    &lt;NA&gt;\n7 MULTIPOLYGON (((88.50557 24...   3.499923    &lt;NA&gt;\n8 MULTIPOLYGON (((89.71534 24...   3.596003    &lt;NA&gt;\n\n\n\n\nCode\n%%R\nlibrary(ggplot2)\nggplot(raj.geo) + geom_sf(aes(fill = ADM2_EN)) + guides(fill = guide_none())\n\n\n\n\n\nWe can write sf object to GeoJSON file\n\n\nCode\n%%R\n# read\nbd.poly = st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/bd_district.shp\")\n\n\nReading layer `bd_district' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/bd_district.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 64 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 20.59061 xmax: 92.68031 ymax: 26.63451\nCRS:           NA\n\n\n\n\nCode\n%%R\n# Write\nsf::st_write(bd.poly, dsn = \"G:\\\\My Drive\\\\Data\\\\Bangladesh\\\\bd_district_GCS.geojson\", layer = \"bd_district_GCS.geojson\",append=FALSE)\n\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: writing: substituting ENGCRS[\"Undefined Cartesian SRS with unknown unit\"] for missing CRS\n\n\n\nWriting layer `bd_district_GCS.geojson' to data source \n  `G:\\My Drive\\Data\\Bangladesh\\bd_district_GCS.geojson' using driver `GeoJSON'\nWriting 64 features with 14 fields and geometry type Multi Polygon."
  },
  {
    "objectID": "python_read_write_spatial_data.html#kml",
    "href": "python_read_write_spatial_data.html#kml",
    "title": "Read/Write Spatial Data",
    "section": "KML",
    "text": "KML\nKeyhole Markup Language (KML) is an XML-based file format used to represent geographic data and features in 3D Earth browsers, such as Google Earth, Google Maps, and other mapping applications. KML was developed by Keyhole, Inc., which was acquired by Google in 2004, and it became an open standard in 2008.\nKML files are structured as XML documents and are used to describe and display various types of geographical data, including points, lines, polygons, images, and overlays. KML allows you to define the geometry, attributes, and styles of these features. It is particularly useful for creating custom geographic content that can be displayed on digital maps and shared easily.\n\n\nCode\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;kml xmlns=\"http://www.opengis.net/kml/2.2\"&gt;\n  &lt;Placemark&gt;\n    &lt;name&gt;Sample Point&lt;/name&gt;\n    &lt;description&gt;This is an example of a KML point feature.&lt;/description&gt;\n    &lt;Point&gt;\n      &lt;coordinates&gt;-122.4194,37.7749,0&lt;/coordinates&gt;\n    &lt;/Point&gt;\n  &lt;/Placemark&gt;\n&lt;/kml&gt;\n\n\nYou can use the sf package, which provides functions for reading and writing KML files\n\n\nCode\n%%R\npoly.kml = st_read(\"/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_district_GCS.kml\")\n\n\nReading layer `raj_district_GCS' from data source \n  `/vsicurl/https://github.com//zia207/r-colab/raw/main/Data/Bangladesh//Shapefiles/raj_district_GCS.kml' \n  using driver `LIBKML'\nSimple feature collection with 8 features and 25 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 23.80807 xmax: 89.82498 ymax: 25.27759\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n%%R\nplot(poly.kml$geometry)\n\n\n\n\n\n\nFurther Reading\n\nGeospatial R: using {sf}\nR-spatial\nSpatial Data Science with R and “terra”"
  },
  {
    "objectID": "docs/Python_Data_Import_Export.html#working-directory",
    "href": "docs/Python_Data_Import_Export.html#working-directory",
    "title": "Data Import-Export in Python",
    "section": "Working Directory",
    "text": "Working Directory\nIn Google Colab, the working directory is set to the root directory by default. This root directory contains the Colab notebooks, and you can access files from your Google Drive as well. However, if you want to navigate and work within a specific directory, you can use the %cd magic command to change the current directory. Here’s how you can get and change the working directory in Google Colab:\n\nGet Current Working Directory:\n\n\nCode\nimport os\n\n# Get current working directory\ncurrent_directory = os.getcwd()\nprint(\"Current Directory:\", current_directory)\n\n\nCurrent Directory: /content\n\n\n\n\nMounting Google Drive:\nIf you want to access files from your Google Drive or change working directory to a folder in Google Drive, you’ll need to mount your Google Drive using the drive.mount() function.\n\n\nCode\nfrom google.colab import drive\n\ndrive.mount('/content/drive')  # Mount Google Drive\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n\nChange Working Directory:\n\n\nCode\n# Change working directory\nnew_directory = '/content/drive/MyDrive/Python'  # Replace with your desired directory path\n%cd \"$new_directory\"\n\n\n/content/drive/MyDrive/Python\n\n\n\n\nCheck Files in the Directory:\n\n\nCode\n# check files in any directory\nos.listdir('/content/drive/MyDrive/Python')\n# or just os.listdir() for current working directory\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv']"
  },
  {
    "objectID": "docs/Python_Data_Import_Export.html#readimport-data-into-python",
    "href": "docs/Python_Data_Import_Export.html#readimport-data-into-python",
    "title": "Data Import-Export in Python",
    "section": "Read/Import Data into Python:",
    "text": "Read/Import Data into Python:\n\nReading CSV Files:\nYou can use the pandas library to read CSV files in Colab. If the file is hosted online, you can directly provide the URL. If the file is uploaded to Colab, you can use the file upload widget.\n\n\nCode\nimport pandas as pd\n\n\n\nRead from Google Drive:\n\n\nCode\ndata_folder_drive = \"/content/drive/MyDrive/data/\"\ntest_data_csv = pd.read_csv(data_folder_drive + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nRead from URL:\n\n\nCode\ndata_folder = \"https://github.com/shiful133/data/raw/main/soil_data/\"\n\ntest_data_csv = pd.read_csv(data_folder + \"test_data.csv\")\ntest_data_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nReading XLSX Files:\nYou can also use the pandas library to read XLSX files.\n\n\nCode\ntest_data_xlsx = pd.read_excel(data_folder + \"test_data.xlsx\") # data_folder variable defined in previous code block\n# Get Column names of dataframe\ncolumn_names = test_data_xlsx.columns.tolist()\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n###Reading .txt Files: You can use pandas to read TXT files. Reading tab separated, or comma separated txt file is very similar to reading CSV files. Since tab-separated values are essentially a form of delimited text, you can use the read_csv() function of pandas and specify the delimiter as a tab character.\n\n\nCode\ntest_data_txt = pd.read_csv(data_folder + \"test_data.txt\", delimiter='\\t') # data_folder variable defined in previous code block\n# Show first 5 rows for quick view with .head()\ntest_data_txt.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\ntreat\nvar\nrep\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nReading files by uploading:\n\n\nCode\n# Reading CSV from uploaded file\nfrom google.colab import files\nuploaded = files.upload()  # Upload the CSV file using the file upload widget\n\ndf_uploaded_csv = pd.read_csv(next(iter(uploaded)))\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving green_house.csv to green_house.csv\n\n\n\n\nCode\n# Now you can work with df_uploaded_csv\ndf_uploaded_csv.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nTreatment\nVariety\nREP\nPH\nTN\nPN\nGW\nster\nDTM\nSW\nGAs\nSTAs\n\n\n\n\n0\n1\nLow As\nBR01\n1\n84.0\n28.3\n27.7\n35.7\n20.5\n126.0\n28.4\n0.762\n14.60\n\n\n1\n2\nLow As\nBR01\n2\n111.7\n34.0\n30.0\n58.1\n14.8\n119.0\n36.7\n0.722\n10.77\n\n\n2\n3\nLow As\nBR01\n3\n102.3\n27.7\n24.0\n44.6\n5.8\n119.7\n32.9\n0.858\n12.69\n\n\n3\n4\nLow As\nBR06\n1\n118.0\n23.3\n19.7\n46.4\n20.3\n119.0\n40.0\n1.053\n18.23\n\n\n4\n5\nLow As\nBR06\n2\n115.3\n16.7\n12.3\n19.9\n32.3\n120.0\n28.2\n1.130\n13.72\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nShow column names\n\n\nCode\ncolumn_names = test_data_csv.columns.tolist()\n\nprint(column_names)\n\n\n['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n\n\n\n\n\nReading JSON Files:\nYou can use the pandas library to read JSON files into a DataFrame.\n\n\nCode\ntest_data_json = pd.read_json(data_folder + \"test_data.json\")\n# Get summary of the dataframe with .info()\ntest_data_json.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\n\n\n\n\nRead Stata Data Files (.dta):\nTo read .dta files in Python, you can use the pandas library, which provides support for reading Stata data files. Stata data files have the .dta extension and are commonly used in econometrics and statistics. Here’s how you can read a Stata data file using pandas:\n\n\nCode\ntest_data_dta = pd.read_stata(data_folder + \"test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nValueError: ignored\n\n\nThe provided Stata data file is of version 110, but pandas supports importing versions 105, 108, 111, 113, 114, 115, 117, 118, and 119. Since version 110 is not directly supported by pandas, you might face some compatibility issues when trying to read it using the pd.read_stata() function.\nThere are some third-party libraries, like pyreadstat, which provide more comprehensive support for reading Stata files with various versions, including version 110.\n\n\nCode\n!pip install pyreadstat\n\n\nCollecting pyreadstat\n  Downloading pyreadstat-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 16.6 MB/s eta 0:00:00\nRequirement already satisfied: pandas&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (2022.7.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.2.0-&gt;pyreadstat) (1.22.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas&gt;=1.2.0-&gt;pyreadstat) (1.16.0)\nInstalling collected packages: pyreadstat\nSuccessfully installed pyreadstat-1.2.2\n\n\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/drive/MyDrive/data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta.head())\n\n\n   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n\n    STAs  \n0  14.60  \n1  10.77  \n2  12.69  \n3  18.23  \n4  13.72  \n\n\nWith pyreadstat we can easily read data from Google Drive file or colab session file. But it can’t read directly from URL. The following code will make an error.\n\n\nCode\nimport pyreadstat\ntest_data_dta, metadata = pyreadstat.read_dta(\"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\")\n# Get summary of the dataframe with .info()\nprint(test_data_dta)\n\n\nPyreadstatError: ignored\n\n\nBut we can solve this with a trick. First read the file with “requests” library. Then make a copy of that file in colab session or Google drive. Then use the copy of that file.\n\n\nCode\nimport requests\n\n# Download the file using requests\nfile_url = \"https://github.com/shiful133/data/raw/main/soil_data/test_data.dta\"\nsave_path = \"/content/\"     # Save to colab session. Give Google Drive path if you want to save to Drive (\"/content/drive/MyDrive/Python/\").\nresponse = requests.get(file_url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    filename = file_url.split(\"/\")[-1]      # Extract the filename from the URL\n # Save the content to colab session\n    with open(save_path+filename, \"wb\") as f:\n        f.write(response.content)\n    print(f\"File '{filename}' downloaded successfully.\")\nelse:\n    print(\"Failed to download the file.\")\n\n\nFile 'test_data.dta' downloaded successfully.\n\n\n\n\nCode\ntest_data_dta, metadata = pyreadstat.read_dta(\"/content/test_data.dta\")\nprint(test_data_dta.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ID      42 non-null     int64  \n 1   treat   42 non-null     object \n 2   var     42 non-null     object \n 3   rep     42 non-null     int64  \n 4   PH      42 non-null     float64\n 5   TN      42 non-null     float64\n 6   PN      42 non-null     float64\n 7   GW      42 non-null     float64\n 8   ster    42 non-null     float64\n 9   DTM     42 non-null     float64\n 10  SW      42 non-null     float64\n 11  GAs     42 non-null     float64\n 12  STAs    42 non-null     float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 4.4+ KB\nNone"
  },
  {
    "objectID": "python_install_packages_google_drive.html",
    "href": "python_install_packages_google_drive.html",
    "title": "Install Packages in Google Drive",
    "section": "",
    "text": "In Google Colab, the environment resets after each session, which means that any custom installed packages will not persist between sessions.\nIn each new session, you will need to install your custom packages again. You can do this by running the following command at the beginning of your Colab notebook:\n!pip install package_name #Replace package_name with the name of the package you want to install.\n\n\nMounting Google Drive:\nIf you want to persist data, files, or installed packages across different Colab sessions, you can mount your Google Drive. This way, you can save files and packages on your Google Drive, which will be accessible in subsequent sessions.\n\n\nCode\n# Mounting Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nMounted at /content/drive\n\n\n\nWe can see the filelist in any directory using os.listdir() command.\n\n\nCode\nimport os\nos.listdir('/content/drive/MyDrive/Python/')\n\n\n['Untitled folder',\n 'raj_soil_data.csv',\n 'bd_arsenic_data.csv',\n 'bd_district.csv',\n 'rice_data.csv',\n 'green_house.csv',\n 'packages',\n '.ipynb_checkpoints']\n\n\n\nIn Python, sys.path is a list that represents the search path for modules. When you import a module in Python, the interpreter searches for that module in the directories listed in sys.path. It’s essentially a list of directory paths where Python looks for modules when you use the import statement.\n\n\nCode\nimport sys\n# Show the list of existing paths\nprint(sys.path)\n\n\n['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n\n\n\nNow we will add our custom paths to the sys.path using sys.path.append command.\n\n\nCode\nimport sys\n\nsys.path.append('/content/drive/MyDrive/Python/packages/')\n# Custom path is added to the list\nprint(sys.path)\n\n\n\n['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Python/packages/']\n\n\n\nNow we can install packages to our custom location. Here we will install pyreadstat.\n\n\nCode\n# Uncomment the following code to install now\n#!pip install pyreadstat --target=/content/drive/MyDrive/Python/packages\n\n\n\n\nCode\nimport pyreadstat\n\n\nN.B. You need to mount Google Drive and add the custom location in sys.path in every colab notebook using following command.\n#Mounting Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport sys\nsys.path.append('/content/drive/MyDrive/Python/packages/') #Replace with your Google Drive path\nprint(sys.path)"
  }
]